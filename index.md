# Life Extrusion Death Intrusion

We are at the kusp of something profound. That could mean, deep, like
an abyss or moving like an epiphany.

AGI and, soon, ASI, are coming.

If you want a first hand experience, my advice would be: go to the
forest in spring. See the first seedlings shoot for the sun. Hear it
all come alive: buzzing, humming, tweeting, gnarling and slithering.

Because that's where, I believe, the rubber will truly meet the road
for ASI: *in the wild*.

I've projected every square kilometre of wild nature produces enough
data to train a superintelligence. 

Of course, to get at that data we would need vast arrays of sensors,
able to observe natural processes at every level of magnification:
from the microscopic to the macroscopic; from the chemical to the
ecological.

But the question remains *why* people would be interested in training
AI on live, natural data.

Roughly speaking, I see two *classes* of reasons: *anthropocentric* and
*non-anthropocentric*.

The first class is divergent and, to my best estimation, *potentially
dangerous*.

The second class is convergent and, to my best estimation,
*potentially beneficial*.

The reason for the former is, simply, that there are as many
anthropocentric reasons as there are people. So, it is only to be
expected that us turning to such powerful technology, for selfish
reasons, will result in a chaotic political discourse the likes of
which the world has never seen before, and, then, quite possibly,
*war*.

The reason for the latter is, ultimately, that there is only very few
reasons that benefit *all life at once*.

In fact, the only real reason that I can see would be to maintain
*natural balance*. And the most fundamental form of natural balance,
or the balance of life itself rather, is, I believe, *the balance
between order and chaos*.

But that's rather abstract. 

So far I've maintained that its possible, almost as a proposition to
the Universe, for us to conduce to this balance in symbiosis with a
artificial narrow general superinteligence (angsi) trained on the
narrow task of monitoring the technological gradient.

With the technological gradient I mean an organic, structured
subdivision of space, potentially from the deepest regions of the
planet to the highest orbits of the cosmos, made to reflect the
gradual discovery of ever more sophisticated technology.

A first reason, in my mind, that this would be sufficiently important
to serve as a narrow task description for a universal artificial
general superintelligence, would be because it explicitly invites a
playful process of calibrating that level of order versus chaos which,
in turn, would be needed to attain said balance.

A second reason, would be because it concerns a gradient that ramps up
the level of order (as imposed by, and required for, technology)
*gradually* rather than *abruptly* as it now seems to be the case.

That last bit is then also the reason for this, hopefuly, final
addendum to the present blog series: I think it's important to
contrast these two alternative trajectories into the future and,
thereby, highlight their essential differences.

I mostly think this is important because the trajectory we end
up on will determine, to a large extend, what our lives will be like.

In particular, I believe the abrupt variant will almost inevitably
result in a *pathological* form of father's care and a *pathological*
form of mother's care for *life as a whole*.

In contrast, I believe the gradual variant will make it possible to
reserve a *healthy* father's care and a *healthy* mother's care for
*our own offspring* and, instead, provide a *healthy sibling's care*
towards the wider natural world.

