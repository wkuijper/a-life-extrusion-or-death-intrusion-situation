# A Life Extrusion or Death Intrusion Situation

We are at the kusp of something profound. That could mean, deep, like
an abyss or moving like an epiphany.

AGI and, soon, ASI, are coming.

If you want a first hand experience, my advice would be: go to the
forest in spring. See the first seedlings shoot for the sun. Hear it
all come alive: buzzing, humming, tweeting, gnarling and slithering.

Because that's where, I believe, the rubber will truly meet the road
for ASI: *in the wild*.

I've projected every square kilometre of wild nature produces enough
data to train a superintelligence. 

Of course, to get at that data we would need vast arrays of sensors,
able to observe natural processes at every level of magnification:
from the microscopic to the macroscopic; from the chemical to the
ecological.

But the question remains *why* people would be interested in training
AI on live, natural data.

Roughly speaking, I see two *classes* of reasons: *anthropocentric* and
*non-anthropocentric*.

The first class is divergent and, to my best estimation, *potentially
dangerous*.

The second class is convergent and, to my best estimation,
*potentially beneficial*.

The reason for the former is, simply, that there are as many
anthropocentric reasons as there are people. So, it is only to be
expected that us turning to such powerful technology, for selfish
reasons, will result in a chaotic political discourse the likes of
which the world has never seen before, and, then, quite possibly,
*war*.

The reason for the latter is, ultimately, that there is only very few
reasons that benefit *all life at once*.

In fact, the only real reason that I can see would be to maintain
*natural balance*. And the most fundamental form of natural balance,
or the balance of life itself rather, is, I believe, *the balance
between order and chaos*.

But that's still rather abstract.

So far I've maintained that its possible, almost as a proposition to
the Universe, for us to conduce to this balance in symbiosis with a
artificial narrow general superinteligence (angsi) trained on the
narrow task of monitoring the technological gradient.

With the technological gradient I mean an organic, structured
subdivision of space, potentially from the deepest regions of the
planet to the highest orbits of the cosmos, made to reflect the
gradual discovery of ever more sophisticated technology.

A first reason, in my mind, that this would be sufficiently important
to serve as a narrow task description for a universal artificial
general superintelligence, would be because it explicitly invites a
playful process of calibrating that level of order versus chaos which,
in turn, would be needed to attain said balance.

A second reason, would be because it concerns a gradient that ramps up
the level of order (as imposed by, and required for, technology)
*gradually* rather than *abruptly* as it now seems to be the case.

That last bit is then also the reason for this, hopefuly, final
addendum to the present blog series: I think it's important to
contrast these two alternative trajectories into the future and,
thereby, highlight their essential differences.

I mostly think this is important because the trajectory we end
up on will determine, to a large extend, what our lives will be like.

In particular, I believe the abrupt variant will almost inevitably
result in us engaging in a *pathological* form of father's care and a
*pathological* form of mother's care for *life as a whole*.

In contrast, I believe the gradual variant will make it possible for
us to reserve a *healthy* father's care and a *healthy* mother's care
for *our own offspring* and, instead, provide a *healthy sibling's
care* towards the wider natural world.

As I hope to show in this post, the title of this blog post is a pun
reflecting the main difference between these two trajectories into the
future.

First, the gradual variant is about putting pressure on life from the
*sides* and letting it form *itself*. Just like a physical extruder
makes products by injecting some plastic material under high pressure
into a mold. 

Except the mold, in this case, will be the Universe itself.

Future, evolved, intelligent life will necesarily evolve under said
pressure, but it's a *sibling* kind of pressure. That is, it's not a
pressure that is the result of *conscious influence* from us to them,
but simply of our continued *presence* on the planet.

Second, the abrupt variant is like putting pressure on life from
*above* and, thereby, shaping it *in our image*. That is what's
commonly called (self) directed evolution and I firmly believe that
would be an existential risk.

Moreover, I believe that to be true, not only for us, but for *all*
life on Earth. 

Hence, I fear, if we follow that trajectory, we become this big
festering, hard shell encasing the planet and, from that moment on,
we'll be faced with the impossible question of what life is allowed to
pass and what life is not allowed to pass our "filter."

I contend that, not only will we cause billions upon billions of
co-lateral victims amongst our sibling species, we will, also, face,
in the end, the inevitable truth that our ways are misguided.

In fact, I predict that the first individual we deem worthy of passing
our "firewall" will be the first one that can put us out of our
misery, hence: *death intrusion*.
